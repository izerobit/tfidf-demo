Amazon Aurora is a high-throughput cloud-native relational data- base offered as part of Amazon Web Services (AWS). One of the more novel differences between Aurora and other relational databases is how it pushes redo processing to a multi-tenant scale-out storage service, purpose-built for Aurora. Doing so reduces networking traffic, avoids checkpoints and crash recovery, enables failovers to replicas without loss of data, and enables fault-tolerant storage that heals without database involvement. Traditional implementations that leverage distributed storage would use distributed consensus al- gorithms for commits, reads, replication, and membership changes and amplify cost of underlying storage. In this paper, we describe how Aurora avoids distributed consensus under most circumstances by establishing invariants and leveraging local transient state. Do- ing so improves performance, reduces variability, and lowers costs.
KEYWORDS
Databases; Distributed Systems; Log Processing; Quorum Models; Fault tolerance; Quorum Sets; Replication; Recovery; Performance
ACM Reference Format:
Alexandre Verbitski, Anurag Gupta, Debanjan Saha, James Corey, Kamal Gupta, Murali Brahmadesam, Raman Mittal, Sailesh Krishnamurthy, Sandor Maurice, and Tengiz Kharatishvilli, Xiaofeng Bao. 2018. Amazon Aurora: On Avoiding Distributed Consensus for I/Os, Commits, and Membership Changes. In SIGMOD’18: 2018 International Conference on Management of Data, June 10–15, 2018, Houston, TX, USA. ACM, New York, NY, USA, 8 pages. https://doi.org/10.1145/3183713.3196937
1 INTRODUCTION
IT workloads are increasingly moving to public cloud providers such as AWS. Many of these workloads require a relational database. Amazon Relational Database Service (RDS) provides a managed service that automates database provisioning, operating system and database patching, backup, point-in-time restore, storage and compute scaling, instance health monitoring, failover, and other capabilities. Our experience managing hundreds of thousands of
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
SIGMOD’18, June 10–15, 2018, Houston, TX, USA
© 2018 Copyright held by the owner/author(s). Publication rights licensed to the Association for Computing Machinery.
ACM ISBN 978-1-4503-4703-7/18/06. . . $15.00 https://doi.org/10.1145/3183713.3196937
database instances in RDS led to the design requirements for Aurora, a high-throughput cloud-native relational database.
In our earlier paper [12], we provided an overview of the design considerations behind Aurora. A key contribution of that paper is to show that, on a fleet-wide basis, it is insufficient to treat failures as independent. At a minimum, it is necessary to consider the correlated impact of the largest unit of failure in addition to the background noise of on-going independent failures. In AWS, the largest unit of failure a system may need to tolerate is an Availability Zone (AZ). An AZ is a subset of a Region that is connected to other AZs through low-latency networking links, but is isolated for most faults, including power, networking, software deployments, flooding, and other phenomena. Aurora supports “AZ+1” failures, resulting in six copies of data, spread across three AZs, a 4/6 write quorum, and a 3/6 read quorum as illustrated in Figure 1. Aurora implements quorum membership changes to handle unexpected failures, heat management, as well as planned software upgrades.
Figure 1: Why are 6 copies necessary ?
Quorum models, such as the one used by Aurora, are rarely used in high-performance relational databases, despite the benefits they provide for availability, durability, and the reduction of latency jitter. We believe this is because the underlying distributed algorithms typically used in these systems – two-phase commit (2PC), Paxos commit, Paxos membership changes, and their variants – can be expensive and incur additional network overheads. The commercial systems we have seen built on these algorithms may scale well but have order-of-magnitude worse cost, performance, and peak to average latency than a traditional relational database running on a single node against local disk.
Quorum break on AZ failure
3/6 read 4/6 write
Quorum survives AZ failure
Industry 3: DB Systems in the Cloud and Open Source
SIGMOD’18, June 10-15, 2018, Houston, TX, USA
In this paper, we show how Aurora leverages only quorum I/Os, locally observable state, and monotonically increasing log order- ing to provide high performance, non-blocking, fault-tolerant I/O, commits, and membership changes. We limit our discussion to single-writer databases with read replicas. The approach described below is extensible to multi-writer databases by ordering writes at database nodes, storage nodes, and using a journal to order opera- tions that span multiple database instances and multiple storage nodes. We describe the following contributions:
(1) How Aurora performs writes using asynchronous flows, es- tablishes local consistency points, uses consistency points for commit processing, and re-establishes them upon crash recovery. (Section 2)
(2) How Aurora avoids quorum reads and how reads are scaled across replicas. (Section 3)
(3) How Aurora uses quorum sets and epochs to make non- blocking reversible membership changes to process failures, grow storage, and reduce costs. (Section 4)
Finally, we briefly survey related work in Section 5 and present concluding remarks in Section 6.
2 MAKING WRITES EFFICIENT
In this section, we review the Aurora storage architecture, how storage is distributed, and our quorum model. We next describe the writes performed by Aurora database instances, and how writes are batched to storage nodes. We then describe how we maintain and advance consistency points across distributed storage and how we re-establish consistency upon crash recovery.
2.1 Aurora System Architecture
Aurora uses a service-oriented architecture where database in- stances are loosely coupled with a multi-tenant scale-out storage service that abstracts a segmented redo log. Each database instance acts as a SQL endpoint and includes most of the components of a traditional database kernel (query processing, access methods, transactions, locking, buffer caching, and undo management). Some database functions, including redo logging, materialization of data blocks, garbage collection, and backup/restore, are offloaded to our storage fleet.
Aurora uses a quorum model, where the database reads from and writes to a subset of copies of data. Formally, a quorum system that employs V copies must obey two rules. First, the read set, Vr , and the write set, Vw , must overlap on at least one copy. This ensures a data item is not read and written by two transactions concurrently and the read quorum contains at least one site with the newest version of the data item. Second, the write set must overlap with priorwritesets,whichcanbedonebyensuringthatVw >V/2.This ensures two write operations from two transactions cannot occur concurrently on the same data item.
Aurora storage is partitioned into segments that individually store the redo log for their portion of the database volume as well as coalesced data blocks. The activities on the storage node are shown in more detail in Figure 2. Foreground activity in a storage node consists of (1) receiving redo records, (2) writing them to an update queue, and acknowledging them back. In background, the
storage node (3) sorts and groups records, (4) gossips with peers to fill in missing records, (5) coalesces them into data blocks, (6) backs them up to Amazon Simple Storage Service (S3), (7) garbage collects backed-up data that will no longer be referenced by an instance, and (8) periodically scrubs data to ensure checksums continue to match the data on disk.
Figure 2: Activity in Aurora Storage Nodes
Segments in Aurora are the minimum unit of failure, with faults monitored and repaired automatically as part of the service. Seg- ments are small, currently representing no more than 10GB of addressable data blocks in the database volume. Segments are repli- cated into protection groups, using V = 6, Vw = 4, and Vr = 3. These six copies are spread across three AZs, with two copies in each of the three AZs. Assuming a 10 second window to detect and repair a segment failure, it would require two independent segment failures as well as an AZ failure in the same 10 second period to lose the ability to repair a quorum. This may seem overly conservative. We don’t think so. AZ failures are a correlated failure of two members in each and every quorum. Across a large fleet, some small number of quorums will be degraded, with some quo- rum member already failed at the time of an AZ failure. The time it takes to repair the failure of this quorum member is the time a database is vulnerable to loss of data with one additional fault.
Protection groups are concatenated together to form a storage volume, which has a one to one relationship with the database instance. While the redo log is segmented and spread across storage nodes, the Log Sequence Number (LSN) space is common across the database volume, monotonically increasing, and allocated by the database instance. This is the key invariant that allows Aurora to avoid distributed consensus for most operations.
2.2 Writes in Aurora
In Aurora, the only writes that cross the network from the database instance to the storage node are redo log records. No data blocks are written from the database instance, not for background writes, not for checkpointing, and not for cache eviction. Instead, redo log application code is run within the storage nodes, materializing blocks in background or on-demand to satisfy a read request.
Industry 3: DB Systems in the Cloud and Open Source
SIGMOD’18, June 10-15, 2018, Houston, TX, USA
Changes to data blocks modify the image in the Aurora buffer cache and add the corresponding redo record to a log buffer. These are periodically flushed to a storage driver to be made durable. Inside the driver, they are shuffled to individual write buffers for each storage node storing segments for the data volume. The dri- ver asynchronously issues writes, receives acknowledgments, and establishes consistency points.
Each log record stores the LSN of the preceding log record in the volume, the previous LSN for the segment, and the previous LSN for the block being modified. The block chain is used by the storage node to materialize individual blocks on demand. The segment chain is used by each storage node to identify records that it has not received and fill in these holes by gossiping with other storage nodes. The full log chain is not needed by an individual storage node but provides a fallback path to regenerate storage volume metadata in case of a disastrous loss of metadata state.
Many database systems boxcar redo log writes to improve through- put. There is a challenge in deciding, with each record, whether to issue the write, to improve latency, or to wait for subsequent records, to improve write efficiency and throughput. Waiting cre- ates performance jitter since early requests entering the boxcar have to wait for later requests or a timeout to fill the request. Jitter is greatest under low load when the boxcar times out.
In Aurora, there are many segments partitioning the redo log and the opportunity to boxcar are lower than with a single unsegmented redo log. Aurora handles this by submitting the asynchronous net- work operation when it receives the first redo log record in the boxcar but continuing to fill the buffer until the network operation executes. This ensures requests are sent without boxcar latency and jitter while packing records together to minimize network packets.
In Aurora, all log writes, including those for commit redo log records, are sent asynchronously to storage nodes, processed asyn- chronously at the storage node, and asynchronously acknowledged back to the database instance.
2.3 Storage Consistency Points and Commits
A traditional relational database working with local disk would write a commit redo log record, boxcar commits together using group commit, and flush the log to ensure that it has been made durable. When working with remote storage, it might use a two- phase commit, or a Paxos commit, or variant, to establish a con- sistency point since there is no individual flush operation across all storage nodes. This is heavyweight and introduces stalls and jitter into the write path. Distributed commit protocols also have failure modalities different from those of quorum writes, making it complex to reason about availability and durability.
As a storage node receives new log records, it may locally ad- vance a Segment Complete LSN (SCL), representing the latest point in time for which it knows it has received all log records. More precisely, SCL is the inclusive upper bound on log records continu- ously linked through the segment chain without gaps. SCL is used by storage nodes as a compact way to identify missing writes when gossiping with their peers in a protection group. Note since any
given write may be lost for any reason we need to tolerate missing writes in the storage nodes.
SCL is sent by the storage node as part of acknowledging a write. Once the database instance observes SCL advance at four of six members of the protection group, it is able to locally advance the Protection Group Complete LSN (PGCL), representing the point at which the protection group has made all writes durable. For example, Figure 3 shows a database with two protection groups, PG1 and PG2, consisting of segments A1-F1 and A2-F2 respectively. In the figure, each solid cell represents a log record acknowledged by a segment, with the odd numbered log records going to PG1 and the even numbered log records going to PG2. Here, PG1’s PGCL is 103 because 105 has not met quorum, PG2’s PGCL is 104 because 106 has not met quorum, and the database’s VCL is 104 which is the highest point at which all previous log records have met quorum.
Figure 3: Storage Consistency Points
For a database, it is not enough for individual writes to be made durable, the entire log chain must be complete to ensure recoverabil- ity. The database instance also locally advances a Volume Complete LSN (VCL) once there are no pending writes preventing PGCL from advancing for one of its protection groups. No consensus is required to advance SCL, PGCL, or VCL – all that is required is bookkeep- ing by each individual storage node and local ephemeral state on the database instance based on the communication between the database and storage nodes.
This is possible because storage nodes do not have a vote in determining whether to accept a write, they must do so. Locking, transaction management, deadlocks, constraints, and other con- ditions that influence whether an operation may proceed are all resolved at the database tier. Processing offloaded to the Aurora storage nodes can progress by executing idempotent operations using local state. This also ensures that failed storage nodes can transparently be repaired without involving the database instance.
A commit is acknowledged by the database to its caller once it is able to affirm that all data modified by the transaction has been durably recorded. A simple way to do so is to ensure that the commit redo record for the transaction, or System Commit Number (SCN), is below VCL. No flush, consensus, or grouping is required.
Aurora must wait to acknowledge commits until it is able to advance VCL beyond the requesting SCN. Typically, this would
 791
Industry 3: DB Systems in the Cloud and Open Source
SIGMOD’18, June 10-15, 2018, Houston, TX, USA
require stalling the worker thread acting upon the user request. In Aurora, user sessions are multiplexed to worker threads as requests are received. When a commit is received, the worker thread writes the commit record, puts the transaction on a commit queue, and returns to a common task queue to find the next request to be pro- cessed. When a driver thread advances VCL, it wakes up a dedicated commit thread that scans the commit queue for SCNs below the new VCL and sends acknowledgements to the clients waiting for commit. There is no induced latency from group commits and no idle time for worker threads.
2.4 Crash Recovery in Aurora
Aurora is able to avoid distributed consensus during writes and commits by managing consistency points in the database instance rather than establishing consistency across multiple storage nodes. But, instances fail. Customers shut them down, resize them, and restore them to older points in time. The time we save in the normal forward processing of commits using local transient state must be paid back by re-establishing consistency upon crash recovery. This is a trade worth making since commits are many orders of magni- tude more common than crashes. Since instance state is ephemeral, the Aurora database instance must be able to construct PGCLs and VCL from local SCL state at storage nodes.
and writes, Aurora increments an epoch in its storage metadata service and records this volume epoch in a write quorum of each protection group comprising the volume. The volume epoch is provided as part of every read or write request to a storage node. Storage nodes will not accept requests at stale volume epochs. This boxes out old instances with previously open connections from accessing the storage volume after crash recovery has occurred. Some systems use leases to establish short term entitlements to access the system, but leases introduce latency when one needs to wait for expiry. Aurora, rather than waiting for a lease to expire, just changes the locks on the door.
No redo replay is required as part of crash recovery since seg- ments are able to generate data blocks on their own. Undo of previ- ously active transactions is required but can occur after the database has been opened in parallel with user activity.
3 MAKING READS EFFICIENT
Reads are one of the few operations in Aurora where threads have to wait. Unlike writes, which can stream asynchronously to storage nodes, or commits, where a worker can move on to other work while waiting for storage to acknowledge, a thread needing a block not in cache typically must wait for the read I/O to complete before it can progress.
In a quorum system, the I/O required for a read is amplified by the size of the read quorum. Network traffic is far higher since one is reading full data blocks, unlike writes, where Aurora only ships log records. A buffer cache miss in Aurora’s quorum model would seem to require a minimum of three read I/Os, and likely five, to mask outlier latency and intermittent unavailability. Read performance in quorum systems compares poorly to traditional replication models where one writes to all copies, enabling a read from just one, though those models have worse write availability.
3.1 Avoiding quorum reads
Aurora uses read views to support snapshot isolation using Multi- Version Concurrency Control (MVCC). A read view establishes a logical point in time before which a SQL statement must see all changes and after which it may not see any changes other than its own. Aurora MySQL does this by establishing the most recent SCN and a list of transactions active as of that LSN. Data blocks seen by a read request must be at or after the read view LSN and back out any transactions either active as of that LSN or started after that LSN. Aurora PostgreSQL also uses MVCC, though writes records out of place, recording the transaction id with each record, and vacuuming old versions periodically. Snapshot isolation is straightforward in a single-node database instance by having a transaction read the last durable version of a database block and apply undo to rollback any changes. One must apply an invariant that undo records may not be purged until all read views have advanced.
Even though Aurora does not write blocks to storage from the database instance, it must support write-ahead logging by ensuring redo log records for dirty blocks have been made durable before discarding the block from cache. This ensures that the latest version of a data block can always be found either in cache or in the cache
 AT CRASH
Log records Gaps
   CRASH
Volume Complete LSN (VCL)
IMMEDIATELY AFTER CRASH RECOVERY
  Figure 4: Log truncation during crash recovery
When opening a database volume, either for crash recovery or for a normal startup, the database instance must be able to reach at least a read quorum for each protection group comprising the volume. The database instance can then locally re-compute PGCLs and VCL for the database by finding read quorum consistency points across SCLs. There may be a ragged edge of updates in particular segments past this point that did not yet meet quorum. These represent partial writes that did not complete and would not have been acknowledged to clients of the database. The database snips off the ragged edge of the log by recording a truncation range that annuls any log records beyond the newly computed VCL (Figure 4). This ensures that, even if in-flight asynchronous operations complete during the process of crash recovery, they are ignored. New redo records after crash recovery are allocated LSNs above the truncation range.
If Aurora is unable to establish write quorum for one of its protection groups, it initiates repair from the available read quorum to rebuild the failed segments. Once the volume is available for reads
792
Industry 3: DB Systems in the Cloud and Open Source
SIGMOD’18, June 10-15, 2018, Houston, TX, USA
or by finding the latest durable version of the block in one of the segments of the protection group that it belongs to.
Aurora does not do quorum reads. Through its bookkeeping of writes and consistency points, the database instance knows which segments have the last durable version of a data block and can request it directly from any of those segments. Avoiding the ampli- fication of read quorums does make Aurora subject to latency when storage nodes are down or jitter when they are busy. We manage this by tracking response time from storage nodes for read requests. The database instance will usually issue a request to the segment with the lowest measured latency, but occasionally also query one of the others in parallel to ensure up to date read latency response times. If a request is taking longer than expected, will issue a read to another storage node and accept whichever one returns first. This caps the latency due to slow or unavailable segments. In an active system, this can be done without request timeouts by inspecting the list of outstanding requests when performing other I/Os.